\section{Experimental Design}

\subsection{Research Questions}
\label{subsec:research_questions}
First, we wanted to see, whether review meetings help the software development process in terms of quality management.

\textbf{RQ1: Are review meetings effective in regards to improving document or code quality?}

Next, we were interested in how efficient these review meetings are in regards to time and monetary requirements.

\textbf{RQ2: How efficient are code review meetings as a means to increasing document or code quality?}

Additionally to direct improvements to the code via the review reports, other studies concerning the topic suggest, that the execution of reviews may provide additional benefits such as knowledge transfer, increased team awareness, or creation of alternate solutions \cite{expecations...}.

\textbf{RQ3: Which positive effects besides improving code quality do review meetings entail?}

%\textit{RQ3:} Which requirements have to be met in order to have effective %review meetings?

%\textit{RQ4:} Which requirements have to be met in order to have efficient %review meetings?

\subsection{Hypotheses}

Initially, we defined three hypotheses and their according null hypotheses.
The first hypothesis refers to RQ1. We wanted to understand if review meetings help improving document quality at all and thus only look at the effectiveness.

\textbf{H1: Review meetings are effective in improving document quality.}

The respective null hypothesis assumes that there is no significant improvement in document quality.

\textbf{H10: Review meetings are not effective in improving document quality.}

Next we were interested in whether or not review meetings are also efficient in improving document quality, since a group of usually five or six software developers is required and thus entails high monetary costs.

\textbf{H2: Review meetings are an efficient way of increasing document quality.}

The null hypothesis for H2 presumes that review meetings are not efficient in improving document quality.

\textbf{H20: Review meetings are not an efficient way of increasing document quality.}

Finally, we defined our third hypothesis concerning the research question of whether or not review meetings bring positive side effects to the table.

\textbf{H3: Review meetings have a positive impact besides the direct improvement to document quality.}

The corresponding null hypothesis, assuming that there are no positive side effects, looks as follows:

\textbf{H30: Review meetings do not have any positive impact besides the direct improvement to document quality.}

\subsection{Design}

Quantitative and qualitative analysis.?
One factor 2 treatment.

For our quantitative analysis we used a one factor two treatment design which you can see in table one. The first treatment is the control treatment of doing no review meetings, while the second treatment is the execution of a review meeting. Since doing no review does not influence the subjects, each group was assigned to both treatments. 

\definecolor{heading}{RGB}{200,200,255}
\definecolor{a}{RGB}{220,220,255}
\definecolor{b}{RGB}{230,230,255}

\begin{table}
\centering
\begin{tabular}{lcr}
  \rowcolor{heading}Subjects & No Review Meeting & Review Meeting \\
  \rowcolor{a}"SoPra" Group 1 & x & x \\
  \rowcolor{b}"SoPra" Group 2 & x & x \\
  \rowcolor{a}... & x & x \\
\end{tabular}
\caption{The one factor two treatment design used for our experiment.}
\end{table}


\subsection{Objects}

The idea of our experiment was that we analyse the review report that result out of the review meetings and compare them to the merged lists of findings from each individual reviewer.

\subsubsection{Review Meetings}

The review meetings are conducted by students of the University of Stuttgart within the scope of the "Software Praktikum". Each review group consists of five people, being three reviewers, one moderator and one scribe, who is also representing the authors. Before the review meeting itself, all reviewers have to inspect the document and create a list of findings using a review tool \cite{TODO:revager Thommy} tool. The reviews are expected to have a duration of TODO:90 minutes. During the reviews, the scribe also uses a review meeting tool to gather the findings and in the end creates a final review report.

\subsubsection{Review Tools}

We looked into the following review tools:
\begin{itemize}
	\item CodeFlow
	\item Gerrit
	\item RevAger
\end{itemize}
CodeFlow is a collaborative code review tool allowing users to directly annotate source code in its viewer and interact with review participants in a live chat model whit recording all the information on code reviews on a central server. At first the developers who want their code to be reviewed create a package with the changed (new, deleted, and modified) files, select the reviewers, write a message to describe the code review, and submit everything to the CodeFlow service whereby CodeFlow then notifies the reviewers about the incoming task via email \cite{Bacchelli:2013:EOC:2486788.2486882}. \\*
Gerrit tightly integrates with test automation and code integration tools where authors upload collections of proposed changes to a software system to a Gerrit server having a set of reviewers comprising either 1) invitation by the author, 2) appointed automatically based on their expertise with the modified system components or 3) self-selected by broadcasting a review request to a mailing list. The reviewers are responsible for critiquing the changes proposed within the patch by leaving comments for the author to address or discuss and also are able to give the changes proposed by a patch revision a score meaning either agreeing or disagreeing with the proposed changes and their level of confidence. The author can reply to comments or address them by producing a new revision of the patch for the reviewers to consider and the verifier can provide comments to describe verification issues that they have encountered during testing. Gerrit also provides Continuous Integration (CI) tools that automatically build and test patches each time a new review request or patch revision is uploaded to Gerrit whereby these generated reports can be automatically appended as a verification report to the code review discussion. Moreover Gerrit allows teams to codify code review and verification criteria that mus tbe satisfied before changes are integrated into upstream version control system repositories \cite{McIntosh:2014:ICR:2597073.2597076}. \\*
We chose the open source review tool RevAger which has been developed in the software internship 2009 at the University of Stuttgart which offers support for the moderator in the planning and initiation phase of the review, the notary in logging of the review results and the developers in the evaluation phase. And since the students of the University of Stuttgart are familiar with this tool the selection seems optimal.
\textit{RevAger:}
\textit{Collaborator}
TODO Thommy: Find more review tools, explain them, decide for RevAger (and find a reason).

\subsection{Experiment Procedure}
This part describes the procedure of the experiment including the prerequisites and assumptions that made this succession of events  reasonable and adequate.
The experiment consists of multiple phases to get both quantitative and qualitative data as a result.
\subsubsection{Prerequisite}
The "Software Praktikum" teams were formed before the actual experiment and became familiar with the theory of review meetings and the review tool \textit{RevAger} beforehand.
They already participated in a review meeting during an earlier phase of their projects, therefore it is reasonable to assume that they know how to do a review meeting properly.
\subsubsection{Preparation phase}
During the preparation phase each team worked on their project independently from each other and unknowingly of our experiment.
They each created a specification document as a result of their requirements analysis.
Because they technically all had to implement the same piece of software, we were able assume that they had a good level of understanding of the underlying problem and the requirements for this piece of software.
At this point, the "Software Praktikum" teams normally would've gotten each other's specification documents to review in a \textit{round-robin} manner.

To create a homogeneous setting for our experiment, we gave our own specification document to all teams instead.
This specification document was created by the authors of this paper independently from the experiment participants, and it is safe to assume that it contains a multitude of errors and mistakes due to it being the very first iteration of a specification document and due to the authors not being fault-free either.
Hence we could do a more realistic evaluation, because all teams reviewed the same document.
\subsubsection{Active phase}
During the active phase of the experiment the actual review meetings took place, which we observed silently.
The participants each presented their findings, which they had worked out singlehandedly before the meeting, in front of their teams.
The resulting quantitative and qualitative data was used to determine answers for the underlying research questions, as described in the following sections.
\subsection{Data collection Procedure}

We classify our data collection in two groups, quantitative and qualitative data collection.
The quantitative data for the control treatment is gathered by taking the findings lists of each reviewer in a review group and merging these lists together, eliminating duplicates.
On the other hand, the quantitative data for the review treatment equals the findings list that results out of the review. Additionally, the RevAger tool automatically measures the elapsed time.
The tool also saves the participants and their respective roles.

The structure of a finding consists of a description, an aspect (what reviewers should have a special focus on, e.g. completeness), a reference (where to find it in the document) and an importance rating. \\

For the qualitative data collection we conduct interviews with all students participating in the review meeting. The interviews are all conducted immediately after every review meeting, in which every participant is interviewed individually.  Before every interview, the participants have to sign a consent form. All audio during the discussion is recorded if agreed by participants and transcribed verbatim for the analysis. Audio records are stored securely and are not intended to be published within this work. Additionally, every participant has to fill a demographic questionnaire, which allows us to gather information about participants semester, education degree, previous review knowledge and review experience etc.. \\
For our interview we prepared various questions which relate to the following subject areas: \textit{theoretical review knowledge}, \textit{practical review experience}, \textit{subjective view of attended review meeting}, \textit{subjective view of benefits and drawbacks of attended review meeting}.\\
The interview question model that we use is the funnel model: Opening the interview with open questions and moving towards more detailed and specific questions. Through this interview design, participants are faced with increasingly difficult questions.  Using this approach ensures to find out more details about the participants attended review meeting. \\
Our interviews are classified as a first degree method to collect data (researchers in direct contact with participants). We use an semistructured interview approach since we are interested in participants individual qualitative and quantitative experience during the review meetings.\\


\subsection{Analysis Procedure}

For the analysis of the findings, we decided to evaluate them manually, due to the fact that there is no objective way of rating the quality of a finding. The evaluation will be executed by three of the study conductors independently and the final result is determined by establishing the median of these.

In order to rate the individual findings, we constructed the following classification scale, which can also be seen in table~\ref{tab:ratings}:
The weighing of the findings are worth 1 for good, 2 for besides error, 3 for main error and 5 for crucial error. If a rating is incorrect, we will instead assign the value for the correct weighting and decrease it by 1.
If a finding itself is incorrect (e.g. claims to have found a mistake, where there is none), the finding will recieve a rating of -1.

Next, we tried to find formula that determines the overall quality of a finding list. It is important to consider both the amount of findings, but also the overall quality of these findings. Altough more findings generally are a positive result, we decided that the overall quality is even more important (e.g. to reduce stacking of rather unimportant "good" findings). Therefore our focus is set more on the overall finding quality, meaning that we rate critical and main errors higher.
We defined the amount of findings $a_L$ by:
\begin{center}

$a_L = \sum_{L}$
\end{center}

Where $L$ is the merged list of all findings, without duplicates.

For the overall quality $q_L$ we determined the arithmetic mean of the accumulated weightings of the findings.

\begin{center}
{$q_L = \frac{1}{a_L} \sum_{f \in L} w_f$}
\end{center}

Where $w_f$ is the weighting (see table~\ref{tab:ratings}) of finding $f$.

In order to evaluate the difference between two finding lists, we defined the following function:

\begin{center}
$f(L_1, L_2) = \frac{a_{L_2} - a_{L_1}}{a_{L_2} + a_{L_1}} + (q_{L_2} - q_{L_1}) $
\end{center}

While in theory $f \in [-7, 7]$ holds with $\frac{a_{L_2} - a_{L_1}}{a_{L_2}} \in [-1, 1]$ and $q_{L_2} - q_{L_1} \in [-6, 6]$, in reality the values we will recieve will be very small. Suppose the reviewers assemble 100 unique findings beforehand and the review meeting results in 5 new findings (which would be a unusually high amount), our first term
would have a value of 0.0244.
As for the second term, suppose the average rating for the findings was 3.0 and 10 classification errors occured, we would receive a final quality rating $q_{L_1}$ of 2.951. Assuming that the review meeting corrects all these classification errors, the second term would add up to 0.049. So overall our improvement is 0.0734. Assuming these values are realistic, our previously mentioned focus on the quality would be given with approximately 2:1.
Considering these values, we now can define our hypotheses.
We define effectiveness as having any improvement from the document without the meeting.
We define our H1 Hypothesis on effectiveness as following:

\begin{center}
\textbf{H1:} $f(L_{pre}, L_{post}) > 0$
\end{center}

Wheres $L_pre$ is the finding list before the review meeting, while $L_post$ describes the finding list that results after the meeting.
The corresponding null hypothesis, assuming that there is no improvement resulting out of a review meeting, looks as follows:

\begin{center}
\textbf{H10:} $f(L_{pre}, L_{post}) \le 0$
\end{center}

As for efficiency, we first have to consider the time and monetary costs that result out of a review meeting. Assuming that a review consists of three reviewers, one moderator, one scribe and an author with an average earnings of 100€/h per developer, an hour would cost approximately 600€, not considering potential costs of the meeting room. Since this research only focuses on the review meetings themselves, we do not consider any costs that arise within a review process outside the meeting itself (e.g. the cost for the reviewers to analyse the specimen). Estimating the cost of an error poses a difficult task. According to \cite{stecklein2004error} the cost of an error in the requirements phase increases by 30 to 70 times, when found in the acceptance testing, where a specification error is most likely found in. We assume that correcting an average error in the specification takes about 3 minutes to correct and thus has an initial cost of about 5€, which will cost about 250€ when found in the acceptance testing.
We assume that this is the average cost for a critical error in order to not overestimate the efficiency. Further we assume that a besides error is worth 100€ and a main error is worth 150€. We define our hypothesis H2, that describes the efficiency of review meetings, as follows:

\begin{center}
\textbf{H2:} $100 * n < 50* \sum_{f \in L_{new}} w_f$
\end{center}

Where $n$ is the amount of developers taking part in the review meeting. $L_{new}$ is the list of newly discovered findings, defined as $L_{new} = L_{post} \setminus L_{pre}$. \\

There are many different ways to conduct a qualitative data analysis. First of all, we want to investigate both research categories, deductive research and inductive research.
In the deductive research approach we compare all collected qualitative data to our previously defined hypotheses in subsection \ref{subsec:research_questions}. This analysis shows us whether the defined hypotheses can be confirmed or not (\textit{Hypothesis confirmation}). Our goal is to extract essential data, in order to create an overview about the base information. Therefore, we analyze all interview transcripts for data organization and categorization. With \textit{Coding}, data from interviews can be summarized by using a short word or phrase that represents participants essential data. We use two different coding approaches: \textit{open coding}: creates labels for big chunks of data and summarizes it; \textit{axial coding}: helps us to identify relationships between \textit{open coding} labels. This especially comes in handy when analyzing if there is a correlation e.g. between participant qualification (e.g. education level) and amount of findings they found in the review meeting. For the qualitative data analysis we use the MaxQD tool in order to add codes to the interview transcripts. \\
Moving away from deductive research, inductive research can create new hypotheses with the data we have collected, which also can be useful when talking about future work (\textit{Hypotheses generation}). Here we used the \textit{constructive grounded theory} approach. The coding procedure is divided in three different coding approaches: \textit{initial coding}: examining data without biasing it; \textit{focused coding}: creating categories from the most used codes; \textit{theoretical coding}: determine relationships between categories.
TODO:not finished yet

\begin{table}
\centering
\begin{tabular}{lr}
  \rowcolor{heading}Classification & Rating \\
  \rowcolor{a}'Good' & 1 \\
  \rowcolor{b}'Besides Error' & 2 \\
  \rowcolor{a}'Main Error' & 3 \\
  \rowcolor{b}'Critical Error' & 5 \\
  \rowcolor{a}'Finding Error' & -1 \\
  \rowcolor{b}'Classification Error Penalty' & -1 \\
\end{tabular}
\caption{Rating table for the evaluation of findings.}
\label{tab:ratings}
\end{table}

\subsection{Validity Procedure}

The review teams were assigned semi-randomly out of all "SoPra" participants with only one condition, being that no "SoPra" team members are within the same review group.

In order to reduce subjectivity of the manual evaluation that will take place when weighting the findings, we decided that three study conductors independently rate these weightings. The median of these three ratings will be taken as the final value.

Each group recieved the specification document three days before their respective review meetings. By doing this we strived for having their memories on the specimen as fresh as possible.
