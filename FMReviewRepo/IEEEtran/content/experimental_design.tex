\section{Experimental Design}

\subsection{Research Questions}
First, we wanted to see, whether review meetings help the software development process in terms of quality management.

\textbf{RQ1: Are review meetings effective in regards to improving document or code quality?}

Next, we were interested in how efficient these review meetings are in regards to time and monetary requirements.

\textbf{RQ2: How efficient are code review meetings as a means to increasing document or code quality?}

Additionally to direct improvements to the code via the review reports, other studies concerning the topic suggest, that the execution of reviews may provide additional benefits such as knowledge transfer, increased team awareness, or creation of alternate solutions \cite{expecations...}.

\textbf{RQ3: Which positive effects besides improving code quality do review meetings entail?}

%\textit{RQ3:} Which requirements have to be met in order to have effective %review meetings?

%\textit{RQ4:} Which requirements have to be met in order to have efficient %review meetings?

\subsection{Hypotheses}

Initially, we defined three hypotheses and their according null hypotheses.
The first hypothesis refers to RQ1. We wanted to understand if review meetings help improving document quality at all and thus only look at the effectiveness.

\textbf{H1: Review meetings are effective in improving document quality.}

The respective null hypothesis assumes that there is no significant improvement in document quality.

\textbf{H10: Review meetings are not effective in improving document quality.}

Next we were interested in whether or not review meetings are also efficient in improving document quality, since a group of usually five or six software developers is required and thus entails high monetary costs.

\textbf{H2: Review meetings are an efficient way of increasing document quality.}

The null hypothesis for H2 presumes that review meetings are not efficient in improving document quality.

\textbf{H20: Review meetings are not an efficient way of increasing document quality.}

Finally, we defined our third hypothesis concerning the research question of whether or not review meetings bring positive side effects to the table.

\textbf{H3: Review meetings have a positive impact besides the direct improvement to document quality.}

The corresponding null hypothesis, assuming that there are no positive side effects, looks as follows:

\textbf{H30: Review meetings do not have any positive impact besides the direct improvement to document quality.}

\subsection{Design}

Quantitative and qualitative analysis.?
One factor 2 treatment.

For our quantitative analysis we used a one factor two treatment design. The first treatment is the control treatment of dong no review meetings, while the second treatment is the execution of a review meeting. Since doing no review does not influence the subjects, each group was assigned to both treatments.

\definecolor{heading}{RGB}{200,200,255}
\definecolor{a}{RGB}{220,220,255}
\definecolor{b}{RGB}{230,230,255}

\begin{table}
\centering
\begin{tabular}{lcr}
  \rowcolor{heading}Subjects & No Review Meeting & Review Meeting \\
  \rowcolor{a}"SoPra" Group 1 & x & x \\
  \rowcolor{b}"SoPra" Group 2 & x & x \\
  \rowcolor{a}... & x & x \\
\end{tabular}
\caption{The one factor two treatment design used for our experiment.}
\end{table}


\subsection{Objects}

The idea of our experiment was that we analyse the review report that result out of the review meetings and compare them to the merged lists of findings from each individual reviewer.

\subsubsection{Review Meetings}

The review meetings are conducted by students of the University of Stuttgart within the scope of the "Software Praktikum". Each review group consists of five people, being three reviewers, one moderator and one scribe, who is also representing the authors. Before the review meeting itself, all reviewers have to inspect the document and create a list of findings using a review tool \cite{TODO:revager Thommy} tool. The reviews are expected to have a duration of TODO:90 minutes. During the reviews, the scribe also uses a review meeting tool to gather the findings and in the end creates a final review report.

\subsubsection{Review Tools}

We looked into the following review tools:
\textit{RevAger:}
\textit{Collaborator}
TODO Thommy: Find more review tools, explain them, decide for RevAger (and find a reason).

\subsection{Experiment Procedure}
TODO: Josip

\subsection{Data collection Procedure}

The data for the control treatment is gathered by taking the findings lists of each reviewer in a review group and merging these lists together, eliminating duplicates.
On the other hand, the data for the review treatment equals the findings list that results out of the review. Additionally, the RevAger tool automatically measures the elapsed time.
The tool also saves the participants and their respective roles.

The structure of a finding consists of a description, an aspect (what reviewers should have a special focus on, e.g. completeness), a reference (where to find it in the document) and an importance rating.

TODO Haris: qualitative data collection


\subsection{Analysis Procedure}

For the analysis of the findings, we decided to evaluate them manually, due to the fact that there is no objective way of rating the quality of a finding. The evaluation will be executed by three of the study conductors independently and the final result is determined by establishing the median of these.

In order to rate the individual findings, we constructed the following classification scale, which can also be seen in table~\ref{tab:ratings}:
The weighing of the findings are worth 1 for good, 2 for besides error, 3 for main error and 5 for crucial error. If a rating is incorrect, we will instead assign the value for the correct weighting and decrease it by 1.
If a finding itself is incorrect (e.g. claims to have found a mistake, where there is none), the finding will recieve a rating of -1.

Next, we tried to find formula that determines the overall quality of a finding list. It is important to consider both the amount of findings, but also the overall quality of these findings. Altough more findings generally are a positive result, we decided that the overall quality is even more important (e.g. to reduce stacking of rather unimportant "good" findings). Therefore our focus is set more on the overall finding quality, meaning that we rate critical and main errors higher.
We defined the amount of findings $a_L$ by:
\begin{center}

$a_L = \sum_{L}$
\end{center}

Where $L$ is the merged list of all findings, without duplicates.

For the overall quality $q_L$ we determined the arithmetic mean of the accumulated weightings of the findings.

\begin{center}
{$q_L = \frac{1}{a_L} \sum_{f \in L} w_f$}
\end{center}

Where $w_f$ is the weighting (see table~\ref{tab:ratings}) of finding $f$.

In order to evaluate the difference between two finding lists, we defined the following function:

\begin{center}
$f(L_1, L_2) = \frac{a_{L_2} - a_{L_1}}{a_{L_2} + a_{L_1}} + (q_{L_2} - q_{L_1}) $
\end{center}

While in theory $f \in [-7, 7]$ holds with $\frac{a_{L_2} - a_{L_1}}{a_{L_2}} \in [-1, 1]$ and $q_{L_2} - q_{L_1} \in [-6, 6]$, in reality the values we will recieve will be very small. Suppose the reviewers assemble 100 unique findings beforehand and the review meeting results in 5 new findings (which would be a unusually high amount), our first term
would have a value of 0.0244.
As for the second term, suppose the average rating for the findings was 3.0 and 10 classification errors occured, we would receive a final quality rating $q_{L_1}$ of 2.951. Assuming that the review meeting corrects all these classification errors, the second term would add up to 0.049. So overall our improvement is 0.0734. Assuming these values are realistic, our previously mentioned focus on the quality would be given with approximately 2:1.
Considering these values, we now can define our hypotheses.
We define effectiveness as having any improvement from the document without the meeting.
We define our H1 Hypothesis on effectiveness as following:

\begin{center}
\textbf{H1:} $f(L_{pre}, L_{post}) > 0$
\end{center}

Wheres $L_pre$ is the finding list before the review meeting, while $L_post$ describes the finding list that results after the meeting.
The corresponding null hypothesis, assuming that there is no improvement resulting out of a review meeting, looks as follows:

\begin{center}
\textbf{H10:} $f(L_{pre}, L_{post}) \le 0$
\end{center}

As for efficiency, we first have to consider the time and monetary costs that result out of a review meeting. Assuming that a review consists of three reviewers, one moderator, one scribe and an author with an average earnings of 100€/h per developer, an hour would cost approximately 600€, not considering potential costs of the meeting room. Since this research only focuses on the review meetings themselves, we do not consider any costs that arise within a review process outside the meeting itself (e.g. the cost for the reviewers to analyse the specimen). Estimating the cost of an error poses a difficult task. According to \cite{stecklein2004error} the cost of an error in the requirements phase increases by 30 to 70 times, when found in the acceptance testing, where a specification error is most likely found in. We assume that correcting an average error in the specification takes about 3 minutes to correct and thus has an initial cost of about 5€, which will cost about 250€ when found in the acceptance testing.
We assume that this is the average cost for a critical error in order to not overestimate the efficiency. Further we assume that a besides error is worth 100€ and a main error is worth 150€. We define our hypothesis H2, that describes the efficiency of review meetings, as follows:

\begin{center}
\textbf{H2:} $100 * n < 50* \sum_{f \in L_{new}} w_f$
\end{center}

Where $n$ is the amount of developers taking part in the review meeting. $L_{new}$ is the list of newly discovered findings, defined as $L_{new} = L_{post} \setminus L_{pre}$.

\begin{table}
\centering
\begin{tabular}{lr}
  \rowcolor{heading}Classification & Rating \\
  \rowcolor{a}'Good' & 1 \\
  \rowcolor{b}'Besides Error' & 2 \\
  \rowcolor{a}'Main Error' & 3 \\
  \rowcolor{b}'Critical Error' & 5 \\
  \rowcolor{a}'Finding Error' & -1 \\
  \rowcolor{b}'Classification Error Penalty' & -1 \\
\end{tabular}
\caption{Rating table for the evaluation of findings.}
\label{tab:ratings}
\end{table}

\subsection{Validity Procedure}

The review teams were assigned semi-randomly out of all "SoPra" participants with only one condition, being that no "SoPra" team members are within the same review group.

In order to reduce subjectivity of the manual evaluation that will take place when weighting the findings, we decided that three study conductors independently rate these weightings. The median of these three ratings will be taken as the final value.

Each group recieved the specification document three days before their respective review meetings. By doing this we strived for having their memories on the specimen as fresh as possible.